import time
import sys
import re
import pandas as pd
import urllib.parse as urlparse
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# í¬ë¡¬ë“œë¼ì´ë²„ ì´ˆê¸° ì„¤ì •
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# ê²°ê³¼ ëˆ„ì  ë¦¬ìŠ¤íŠ¸
info_list = []
comment_dict = {}

# í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ list URL ì½ê¸°  
with open("í˜„íŒ.txt", "r", encoding="utf-8") as f:  #ì›¹íˆ° urlì´ ë“¤ì–´ ìˆëŠ” txt íŒŒì¼ë§Œ ë³€ê²½í•˜ë©´ ë¨ 
    list_urls = [line.strip() for line in f if line.strip()]

for list_url in list_urls:
    try:
        parsed = urlparse.urlparse(list_url)
        novel_id = urlparse.parse_qs(parsed.query)['novelId'][0]

        def get_detail_url(volume_no):
            return f"https://novel.naver.com/webnovel/detail?novelId={novel_id}&volumeNo={volume_no}"

        # ì ‘ì†
        driver.get(list_url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # ì›¹ì†Œì„¤ ê¸°ë³¸ ì •ë³´
        title_tag = soup.find('h2', class_='title')
        title = title_tag.text.strip() if title_tag else f"ì‘í’ˆ_{novel_id}"

        genre = soup.find('span', class_='item')
        genre = genre.text.strip() if genre else 'ì¥ë¥´ ì—†ìŒ'

        score_tag = soup.select_one('span.score_area')
        score = score_tag.get_text(strip=True).replace('ë³„ì ', '') if score_tag else 'ë³„ì  ì—†ìŒ'

        download_tag = soup.select_one('span.count')
        download = download_tag.get_text(strip=True) if download_tag else 'ë‹¤ìš´ë¡œë“œ ìˆ˜ ì—†ìŒ'

        concern_tag = soup.find('span', id='concernCount')
        concern = concern_tag.text.strip() if concern_tag else 'ê´€ì‹¬ ìˆ˜ ì—†ìŒ'

        # ëŒ“ê¸€ ìˆ˜ì§‘
        comments = []
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'li.wcc_CommentItem__root'))
            )
            for _ in range(20):
                try:
                    more_button = WebDriverWait(driver, 3).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'button.wcc_CommentMore__more'))
                    )
                    driver.execute_script("arguments[0].scrollIntoView(true);", more_button)
                    time.sleep(0.5)
                    driver.execute_script("arguments[0].click();", more_button)
                    time.sleep(1)
                except:
                    break

            last_height = driver.execute_script("return document.body.scrollHeight")
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            comment_items = soup.select('li.wcc_CommentItem__root')
            for item in comment_items:
                body = item.select_one('p.wcc_CommentBody__root span')
                if body:
                    comments.append(body.get_text(strip=True))
        except:
            pass

        # ëŒ“ê¸€ ì €ì¥ìš© dict ì— ì¶”ê°€
        comment_dict[title] = comments

        # íšŒì°¨ ë³¸ë¬¸ ìˆ˜ì§‘
        try:
            total_episode_tag = soup.select_one("span.past_number")
            total_episode = int(re.search(r'\d+', total_episode_tag.text).group()) if total_episode_tag else 0

            mid = total_episode // 2
            volume_nos = list(range(1, 4)) + list(range(mid - 1, mid + 2)) + list(range(total_episode - 2, total_episode + 1))
            episode_texts = {}

            for vol in volume_nos:
                driver.get(get_detail_url(vol))
                try:
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.detail_view_content.ft15'))
                    )
                    detail_soup = BeautifulSoup(driver.page_source, "html.parser")
                    content_div = detail_soup.select_one('div.detail_view_content.ft15')
                    paragraphs = content_div.find_all('p')
                    text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                    episode_texts[vol] = text
                except:
                    episode_texts[vol] = "ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨"
        except:
            episode_texts = {}

        # ì´ˆ/ì¤‘/í›„ 3í™”
        first_3 = "\n\n".join([episode_texts.get(i, "") for i in range(1, 4)])
        middle_3 = "\n\n".join([episode_texts.get(i, "") for i in range((total_episode // 2) - 1, (total_episode // 2) + 2)])
        last_3 = "\n\n".join([episode_texts.get(i, "") for i in range(total_episode - 2, total_episode + 1)])

        # ì •ë³´ ëˆ„ì 
        info_list.append({
            "ì œëª©": title,
            "ì¥ë¥´": genre,
            "ë³„ì ": score,
            "ë‹¤ìš´ë¡œë“œ ìˆ˜": download,
            "ê´€ì‹¬ ìˆ˜": concern,
            "ì´ˆë°˜ 2í™”": first_3,
            "ì¤‘ê°„ 2í™”": middle_3,
            "ë§ˆì§€ë§‰ 2í™”": last_3
        })

        print(f"ì²˜ë¦¬ ì™„ë£Œ: {title}")

    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {list_url}")
        print(e)

# ë“œë¼ì´ë²„ ì¢…ë£Œ
driver.quit()

# CSVë¡œ ì €ì¥
df_info = pd.DataFrame(info_list)
df_info.to_csv("í˜„íŒ_ë³¸ë¬¸ì •ë³´.csv", index=False, encoding="utf-8-sig")
print("ğŸ“˜ ì „ì²´ ë³¸ë¬¸ CSV ì €ì¥ ì™„ë£Œ")

# ëŒ“ê¸€ dict â†’ DataFrame (column = ì œëª©)
df_comment = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in comment_dict.items()]))
df_comment.to_csv("í˜„íŒ_ëŒ“ê¸€.csv", index=False, encoding="utf-8-sig")
print("ğŸ’¬ ì „ì²´ ëŒ“ê¸€ CSV ì €ì¥ ì™„ë£Œ")
